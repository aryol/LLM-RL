name: ???
tokenizer: null
train_files: ???
val_files: ???
prompt_key: prompt
train_dataset_type: adaptive # base or adaptive
# reward_fn_key: data_source
max_prompt_length: ???
max_response_length: ???
train_batch_size: ???
val_batch_size: null # DEPRECATED: Validation datasets are sent to inference engines as a whole batch, which will schedule the memory themselves
return_raw_input_ids: False  # This should be set to true when the tokenizer between policy and rm differs
return_raw_chat: False
shuffle: True
filter_overlong_prompts: True # for large-scale dataset, filtering overlong prompts could be timeconsuming. You cat set the filter_overlong_prompts_workers to use multiprocessing to speed up.
filter_overlong_prompts_workers: 1
truncation: error
image_key: images