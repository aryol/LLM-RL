# @package _global_

defaults:
  - override /callbacks/curriculum_update_callback: just_log
  - override /dataset_wrapper: default
  - override /trainer: PPO
  - override /task: ???
  - override /model: ???

# include the params we'd like to possibly override depending on model or our objects of studies.
trainer:
  args:
    max_steps: 4500
    eval_steps: 4
    
    gradient_accumulation_steps: 2
    per_device_train_batch_size: 2
    per_device_eval_batch_size: 8


# accelerator:
#   num_processes: 1
#   compute_environment: LOCAL_MACHINE
#   debug: false
#   deepspeed_config:
#     deepspeed_multinode_launcher: standard
#     offload_optimizer_device: none
#     offload_param_device: none
#     zero_stage: 2
#   distributed_type: DEEPSPEED
#   downcast_bf16: 'no'
#   machine_rank: 0
#   main_training_function: main
#   mixed_precision: bf16
#   num_machines: 1
#   rdzv_backend: static
#   same_network: true
#   tpu_env: []
#   tpu_use_cluster: false
#   tpu_use_sudo: false
#   use_cpu: false
    

