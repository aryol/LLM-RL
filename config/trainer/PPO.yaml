# _target_: trl.PPOTrainer
_target_: src.trainers.PPOTrainerWithCustomReward

args:
  _target_: trl.PPOConfig

  max_steps: 4500
  per_device_train_batch_size: 3
  per_device_eval_batch_size: 3
  gradient_accumulation_steps: 2
  gradient_checkpointing: true
  gradient_checkpointing_kwargs:
    use_reentrant: false
    
  learning_rate: 5.0e-7 # 1.0e-6 as in the deepseek math paper 5-e7 from https://hijkzzz.notion.site/unraveling-rlhf-and-its-variants-engineering-insights#147d9a33ecc9806090f3d5c749d31f05
  lr_scheduler_type: cosine
  warmup_ratio: 0.03
  
  output_dir: ${log_dir}/${task.task_name}-${model.model_name}/${now:%Y-%m-%d}_${now:%H-%M-%S}
  run_name: ${task.task_name}-${model.model_name}

  bf16: true
  tf32: true
  logging_strategy: steps
  logging_steps: 2
  report_to: wandb
  save_strategy: "steps"
  save_steps: 10
  seed: 42
  torch_compile: false

  eval_strategy: "steps"
  eval_steps: 125
  do_eval: true
  do_train: true

  # PPO specific parameters starting here:
  per_gpu_train_batch_size: None
  per_gpu_eval_batch_size: None

  # somehow these are the only parameters described in the trl documentation
  stop_token: eos
  response_length: 256
  exp_name: ${.run_name}
  reward_model_path: null
  model_adapter_name: null
  ref_adapter_name: null
  num_ppo_epochs: 40
  whiten_rewards: false
  kl_coef: 0.05
  cliprange: 0.2
  vf_coef: 0.1
  cliprange_value: 0.2
  gamma: 1.0
  lam: 0.95
  ds3_gather_for_generation: True 