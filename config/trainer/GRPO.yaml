_target_: trl.GRPOTrainer

args:
  _target_: trl.GRPOConfig

  max_steps: 4500
  per_device_train_batch_size: 8
  per_device_eval_batch_size: 8
  gradient_accumulation_steps: 8
  gradient_checkpointing: true
  gradient_checkpointing_kwargs:
    use_reentrant: false
    
  learning_rate: 5.0e-7 # 1.0e-6 as in the deepseek math paper 5-e7 from https://hijkzzz.notion.site/unraveling-rlhf-and-its-variants-engineering-insights#147d9a33ecc9806090f3d5c749d31f05
  lr_scheduler_type: cosine
  warmup_ratio: 0.03
  # GRPO specific parameters
  beta: 0.001 # 0.04 as in the deepseek math paper 0.001 from https://hijkzzz.notion.site/unraveling-rlhf-and-its-variants-engineering-insights#147d9a33ecc9806090f3d5c749d31f05
  max_prompt_length: 6144
  max_completion_length: 1024
  num_generations: 8
  output_dir: ${log_dir}/${task.task_name}-${model.model_name}/${now:%Y-%m-%d}_${now:%H-%M-%S}
  run_name: ${task.task_name}-${model.model_name}

  bf16: true
  tf32: true
  logging_strategy: steps
  logging_steps: 2
  report_to: wandb
  save_strategy: "steps"
  save_steps: 20
  save_only_model: true
  load_best_model_at_end: false
  seed: 42
  torch_compile: false

  eval_strategy: "steps"
  eval_steps: 125
  do_eval: true
  do_train: true