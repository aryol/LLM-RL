# @package _global_

defaults:
  - _self_
  - curriculum_strategy: null
  - task: ???
  - model: ???
  - experiment: ???

# Training arguments
training_args:
  _target_: trl.GRPOConfig
  max_steps: 450
  per_device_train_batch_size: 8
  per_device_eval_batch_size: 16
  gradient_accumulation_steps: 1
  gradient_checkpointing: true
  gradient_checkpointing_kwargs:
    use_reentrant: false
  learning_rate: 5.0e-7 # 1.0e-6 as in the deepseek math paper 5-e7 from https://hijkzzz.notion.site/unraveling-rlhf-and-its-variants-engineering-insights#147d9a33ecc9806090f3d5c749d31f05
  lr_scheduler_type: cosine
  warmup_ratio: 0.03
  # GRPO specific parameters
  beta: 0.001 # 0.04 as in the deepseek math paper 0.001 from https://hijkzzz.notion.site/unraveling-rlhf-and-its-variants-engineering-insights#147d9a33ecc9806090f3d5c749d31f05
  max_prompt_length: 6144
  max_completion_length: 1024
  num_generations: 2
  use_vllm: false
  # vllm_device: "cuda:7"
  vllm_gpu_memory_utilization: 0.8
  output_dir: runs/qwen-2.5-05b-r1-gsm8k
  bf16: true
  tf32: true
  
# Logging arguments
logger_configs:
  logging_strategy: steps
  logging_steps: 2
  report_to:
  - wandb
  save_strategy: "steps"
  save_steps: 25
  seed: 42
  experiment_name: "progressive-RL"

eval_strategy: "steps"
eval_steps: 30
do_eval: true
do_train: true
