# @package _global_

defaults:
  - _self_
  - callbacks: default
  - task: ???
  - model: ???
  - experiment: qwen_gsm8k

log_dir: ${oc.env:PROJECT_ROOT}/logs

# Training arguments
training_args:
  max_steps: 900
  per_device_train_batch_size: 8
  per_device_eval_batch_size: 16
  gradient_accumulation_steps: 8
  gradient_checkpointing: true
  gradient_checkpointing_kwargs:
    use_reentrant: false
  learning_rate: 5.0e-7 # 1.0e-6 as in the deepseek math paper 5-e7 from https://hijkzzz.notion.site/unraveling-rlhf-and-its-variants-engineering-insights#147d9a33ecc9806090f3d5c749d31f05
  lr_scheduler_type: cosine
  warmup_ratio: 0.03
  # GRPO specific parameters
  beta: 0.001 # 0.04 as in the deepseek math paper 0.001 from https://hijkzzz.notion.site/unraveling-rlhf-and-its-variants-engineering-insights#147d9a33ecc9806090f3d5c749d31f05
  max_prompt_length: 6144
  max_completion_length: 1024
  num_generations: 8
  use_vllm: true
  vllm_device: "cuda:7"
  vllm_gpu_memory_utilization: 0.8
  output_dir: ${log_dir}/${task.task_name}-${model.model_name}/${now:%Y-%m-%d}_${now:%H-%M-%S}
  run_name: ${task.task_name}-${model.model_name}
  bf16: true
  tf32: true
  logging_strategy: steps
  logging_steps: 2
  report_to:
  - wandb
  save_strategy: "steps"
  save_steps: 80
  seed: 42
  torch_compile: false

  eval_strategy: "steps"
  eval_steps: 40
  do_eval: true
  do_train: true

wandb_config:
  name: ${model.model_name}
  project: progressive_rl_on_${task.task_name}
  dir: ${log_dir}/${task.task_name}-${model.model_name}/${now:%Y-%m-%d}_${now:%H-%M-%S}
  notes: null

hydra:
  run:
    dir: ${log_dir}/${task.task_name}-${model.model_name}/${now:%Y-%m-%d}_${now:%H-%M-%S}
  sweep:
    dir: ${log_dir}/${task.task_name}-${model.model_name}/${now:%Y-%m-%d}_${now:%H-%M-%S}
    subdir: ${hydra.job.num}
